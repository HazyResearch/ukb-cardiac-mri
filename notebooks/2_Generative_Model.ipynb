{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Ensure that you have `train` and `unknown` vocab matrices.\n",
    "To do this, run `1_Preprocess.ipynb` for both your small set of ground truth images and your larger set of unknown-label images to generate `train_vocab_matrix` and `unknown_vocab_matrix` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vocab_matrix = np.load('/dfs/scratch0/vschen/mri-data/relative_train_vocab_matrix.npy')\n",
    "train_labels = np.load('/dfs/scratch0/vschen/mri-data/train_labels.npy')\n",
    "unknown_vocab_matrix = np.load('/dfs/scratch0/vschen/mri-data/relative_4k_unknown_vocab_matrix.npy')\n",
    "\n",
    "print \"Number of Labeled Datapoints: \", train_vocab_matrix.shape[1]\n",
    "print \"Number of Unlabeled Datapoints: \", full_unknown_vocab_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Add Coral and Numbskull to your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../ukb/weak_supervision/numbskull') \n",
    "sys.path.append('../ukb/weak_supervision/coral') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct primitive matrices from your vocab matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimitiveObject(object):\n",
    "\n",
    "    def save_primitive_matrix(self,primitive_mtx):\n",
    "        self.primitive_mtx = primitive_mtx\n",
    "        self.discrete_primitive_mtx = primitive_mtx\n",
    "        self.num_primitives = np.shape(self.primitive_mtx)[1]\n",
    "    \n",
    "    def save_primitive_names(self,names):\n",
    "        self.primitive_names = names\n",
    "        if len(self.primitive_names) != self.num_primitives:\n",
    "            Exception('Incorrect number of Primitive Names')\n",
    "            \n",
    "def create_primitives(vocab_matrix):\n",
    "    m = 5\n",
    "    num_examples = vocab_matrix.shape[1]\n",
    "    primitive_mtx = np.zeros((num_examples, m))\n",
    "    for i in range(num_examples):\n",
    "        primitive_mtx[i, 0] = vocab_matrix[0, :][i] # area\n",
    "        primitive_mtx[i, 1] = vocab_matrix[1, :][i] # eccentricity\n",
    "        primitive_mtx[i, 2] = vocab_matrix[6, :][i] # perimeter\n",
    "        primitive_mtx[i, 3] = vocab_matrix[8, :][i] # intensity\n",
    "    \n",
    "    \n",
    "    primitive_mtx[:, 4] = primitive_mtx[:, 0]/(primitive_mtx[:, 2]**2.) # ratio\n",
    "    P = PrimitiveObject()\n",
    "    P.save_primitive_matrix(primitive_mtx)\n",
    "    return P\n",
    "\n",
    "def create_primitives_bsa(vocab_matrix, normal_matrix):\n",
    "    m = 5\n",
    "    num_examples = vocab_matrix.shape[1]\n",
    "    primitive_mtx = np.zeros((num_examples, m))\n",
    "    for i in range(num_examples):\n",
    "        primitive_mtx[i, 0] = vocab_matrix[0, :][i] # area\n",
    "        primitive_mtx[i, 1] = vocab_matrix[1, :][i] # eccentricity\n",
    "        primitive_mtx[i, 2] = vocab_matrix[6, :][i] # perimeter\n",
    "        primitive_mtx[i, 3] = vocab_matrix[8, :][i] # intensity\n",
    "    \n",
    "    \n",
    "    primitive_mtx[:, 4] = normal_matrix[0,:]/(normal_matrix[6,:]**2.) # ratio\n",
    "    P = PrimitiveObject()\n",
    "    P.save_primitive_matrix(primitive_mtx)\n",
    "    return P\n",
    "\n",
    "P_train = create_primitives(train_vocab_matrix)\n",
    "P_unknown = create_primitives(unknown_vocab_matrix)\n",
    "\n",
    "primitive_names = ['area', 'eccentricity', 'perimeter', 'intensity', 'ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write heuristic functions over your chosen primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lf_area(area):\n",
    "    if area >= 2.13:\n",
    "        return -1 \n",
    "    if area <= 0.9: \n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def lf_eccentricity(eccentricity):\n",
    "    if eccentricity >= 0.011: \n",
    "        return 1 \n",
    "    if eccentricity <= 0.015:\n",
    "        return -1  \n",
    "    return 0\n",
    "        \n",
    "def lf_perimeter(perimeter):\n",
    "    if perimeter <= 0.46: \n",
    "        return 1 \n",
    "    return 0\n",
    "    \n",
    "def lf_intensity(intensity):\n",
    "    if intensity >= 3.05: \n",
    "        return 1\n",
    "    if intensity <= 2.0: \n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def lf_ratio(ratio):\n",
    "    if ratio >= 4.15: \n",
    "        return -1\n",
    "    if ratio <= 3.7:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate a label matrix for the train set by applying your labeling functions to each set of primitives\n",
    "In this step, we depend on the Coral paradigm's ability to automatically find dependencies between primitives that we can leverage in our generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from coral.static_analysis.dependency_learning import find_dependencies\n",
    "\n",
    "L_names = [lf_area, lf_eccentricity, lf_perimeter, lf_intensity, lf_ratio]\n",
    "L_deps = find_dependencies(L_names, primitive_names)\n",
    "\n",
    "num_examples_train = P_train.primitive_mtx.shape[0]\n",
    "L = np.zeros((len(L_names), num_examples_train))\n",
    "for i in xrange(num_examples_train):\n",
    "    for j in xrange(5):\n",
    "        vocab_elems = P_train.primitive_mtx[i,L_deps[j]]\n",
    "        L[j,i] = L_names[j](*vocab_elems)\n",
    "unlabeled = np.sum(np.abs(L), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize LF performance on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "unlabeled = np.sum(np.abs(L), axis=0)\n",
    "print 'Coverage:', 1-float(np.sum(unlabeled == 0))/len(unlabeled)\n",
    "\n",
    "total = float(num_examples_train)\n",
    "stats_table = np.zeros((5,6))\n",
    "for i in range(5):    \n",
    "    predicted = L[i, :]\n",
    "    stats_table[i,5] = precision_score(predicted[predicted != 0], train_labels[predicted != 0])\n",
    "    stats_table[i,4] = recall_score(predicted[predicted != 0], train_labels[predicted != 0])\n",
    "    stats_table[i,3] = f1_score(predicted[predicted != 0], train_labels[predicted != 0])\n",
    "    stats_table[i,2] = np.sum(L[i,:] == train_labels)/float(np.sum(L[i,:] != 0))\n",
    "    try: \n",
    "        stats_table[i,1] = roc_auc_score(predicted[predicted != 0], train_labels[predicted != 0])\n",
    "    except ValueError as err: \n",
    "        stats_table[i,1] = None\n",
    "        print 'LF:', i, err.args\n",
    "    stats_table[i,0] = np.sum(np.abs(L[i,:]) != 0)/total\n",
    "    \n",
    "stats_table = pd.DataFrame(stats_table, index = [i.__name__ for i in L_names], columns = [\"Coverage\", \"AUC\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
    "stats_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Discretize Primitives for Gibbs Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def primitives_to_discrete(P, L_names):\n",
    "    num_examples = P.primitive_mtx.shape[0]\n",
    "    code = discretize_primitives(L_names)\n",
    "    \n",
    "    P.discrete_primitive_mtx = np.zeros((num_examples,len(primitive_names)))\n",
    "    for i in range(num_examples):\n",
    "        for j in range(len(code)):\n",
    "            exec(code[j])\n",
    "\n",
    "    P.discrete_primitive_mtx = P.discrete_primitive_mtx.astype(int)\n",
    "    cardinality = []\n",
    "    for v_idx in xrange(P.num_primitives):\n",
    "        cardinality.append(int(np.max(P.discrete_primitive_mtx[:,v_idx])+1))\n",
    "    return cardinality\n",
    "\n",
    "cardinality_train = primitives_to_discrete(P_train, L_names)\n",
    "cardinality_unknown = primitives_to_discrete(P_unknown, L_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learn Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coral_model = CoralModel()\n",
    "deps = ds.select(P.discrete_primitive_mtx, cardinality, L_deps, \"HEART_MRI\", threshold=thresh)\n",
    "coral_model.train(P_unknown.discrete_primitive_mtx, cardinality_unknown, L_deps, MRI_UDF_OFFSET, deps=list(deps), epochs=1000, burn_in=0, reg_type=1, reg_param=0.01)\n",
    "marginals_deps = coral_model.marginals(P_unknown.discrete_primitive_mtx, cardinality_train, L_deps, MRI_UDF_OFFSET, deps=list(deps), epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure and Visualize probabilistic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_auc = roc_auc_score(train_labels[unlabeled != 0], marginals_deps[unlabeled != 0])\n",
    "print ('auc:', curr_auc)\n",
    "\n",
    "plt.hist(marginals_deps)\n",
    "print (\"Indices: \", np.where(marginals_deps >= 0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save out probabilistic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('marginals.npy', marginals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
